#!/usr/bin/env python3
"""
Comprehensive Integration Tests - End-to-end pipeline testing.

Tests complete workflows including error recovery, API fallbacks,
and cross-module interactions.
"""

import pytest
import json
import tempfile
from pathlib import Path
from unittest.mock import Mock, patch, MagicMock
from datetime import datetime, timedelta


class TestFullPipelineExecution:
    """Test complete pipeline execution from collection to deployment."""

    @pytest.fixture
    def mock_trends_data(self):
        """Sample trends data for testing."""
        return [
            {
                'title': 'Major Tech Announcement',
                'source': 'hackernews',
                'url': 'https://example.com/tech',
                'score': 150,
                'timestamp': datetime.now(),
                'keywords': ['tech', 'announcement', 'major']
            },
            {
                'title': 'Science Breakthrough',
                'source': 'science_nature',
                'url': 'https://example.com/science',
                'score': 120,
                'timestamp': datetime.now(),
                'keywords': ['science', 'breakthrough', 'research']
            },
            {
                'title': 'World News Update',
                'source': 'news_reuters',
                'url': 'https://example.com/world',
                'score': 100,
                'timestamp': datetime.now(),
                'keywords': ['world', 'news', 'update']
            },
        ]

    @pytest.fixture
    def mock_images_data(self):
        """Sample images data for testing."""
        return [
            {
                'id': 'img1',
                'url_large': 'https://example.com/img1.jpg',
                'query': 'technology innovation',
                'description': 'Modern tech workspace',
                'width': 1920,
                'height': 1080
            },
            {
                'id': 'img2',
                'url_large': 'https://example.com/img2.jpg',
                'query': 'science research',
                'description': 'Laboratory equipment',
                'width': 1600,
                'height': 900
            },
        ]

    def test_pipeline_with_minimal_data(self, mock_trends_data, mock_images_data):
        """Test pipeline can execute with minimum required data."""
        assert len(mock_trends_data) >= 3  # MIN_TRENDS requirement
        assert len(mock_images_data) >= 2

        # Simulate pipeline steps
        trends_collected = len(mock_trends_data)
        images_fetched = len(mock_images_data)

        assert trends_collected >= 3
        assert images_fetched >= 2

    def test_quality_gates_enforced(self, mock_trends_data):
        """Test that quality gates prevent bad deployments."""
        from config import MIN_TRENDS, MIN_FRESH_RATIO

        # Check minimum trends gate
        assert len(mock_trends_data) >= MIN_TRENDS

        # Check freshness ratio
        now = datetime.now()
        fresh_trends = [
            t for t in mock_trends_data
            if (now - t['timestamp']) < timedelta(hours=24)
        ]
        freshness_ratio = len(fresh_trends) / len(mock_trends_data)

        assert freshness_ratio >= MIN_FRESH_RATIO


class TestErrorRecovery:
    """Test error recovery and graceful degradation."""

    def test_api_fallback_chain(self):
        """Test that API calls fall back through provider chain."""
        # Simulate Groq failure â†’ OpenRouter fallback
        providers = ['groq', 'openrouter', 'google']
        attempt_count = 0

        for provider in providers:
            attempt_count += 1
            if provider == 'openrouter':
                # Simulate success on second provider
                break

        assert attempt_count == 2  # Should succeed on second try
        assert attempt_count < len(providers)  # Didn't exhaust all providers

    def test_image_cache_fallback(self):
        """Test image fetching falls back to cache when APIs fail."""
        # Simulate API failure
        api_failed = True
        cached_images = [{'id': 'cached1', 'url': 'cache://img1.jpg'}]

        if api_failed and cached_images:
            images = cached_images
        else:
            images = []

        assert len(images) > 0  # Should have cached fallback

    def test_gradient_fallback_when_no_images(self):
        """Test gradient placeholder when no images available."""
        api_images = []
        cached_images = []

        if not api_images and not cached_images:
            # Should generate gradient placeholder
            placeholder = {
                'type': 'gradient',
                'colors': ['#667eea', '#764ba2']
            }
        else:
            placeholder = None

        assert placeholder is not None
        assert placeholder['type'] == 'gradient'


class TestTopicPageGeneration:
    """Test topic page generation with various scenarios."""

    @pytest.fixture
    def topic_trends(self):
        """Sample trends for different topics."""
        return {
            'tech': [
                {'title': 'HN Story', 'source': 'hackernews'},
                {'title': 'Lobsters Story', 'source': 'lobsters'},
                {'title': 'Tech News', 'source': 'tech_verge'},
                {'title': 'GitHub Trend', 'source': 'github_trending'},
            ],
            'world': [
                {'title': 'Reuters News', 'source': 'news_reuters'},
                {'title': 'BBC Update', 'source': 'news_bbc'},
                {'title': 'Wiki Event', 'source': 'wikipedia'},
            ],
            'science': [
                {'title': 'Science Article', 'source': 'science_nature'},
            ]
        }

    def test_generates_pages_with_sufficient_stories(self, topic_trends):
        """Test that pages are generated when enough stories exist."""
        from scripts.topic_page_generator import should_generate_topic_page

        assert should_generate_topic_page(topic_trends['tech'], min_stories=3)
        assert should_generate_topic_page(topic_trends['world'], min_stories=3)

    def test_skips_pages_with_few_stories(self, topic_trends):
        """Test that pages are skipped when insufficient stories."""
        from scripts.topic_page_generator import should_generate_topic_page

        assert not should_generate_topic_page(topic_trends['science'], min_stories=3)

    def test_unique_images_across_topics(self):
        """Test that each topic page gets a unique hero image."""
        used_image_ids = set()
        images = [
            {'id': 'img1', 'query': 'tech'},
            {'id': 'img2', 'query': 'world'},
            {'id': 'img3', 'query': 'science'},
        ]

        # Simulate image selection for 3 topics
        for topic in ['tech', 'world', 'science']:
            available = [img for img in images if img['id'] not in used_image_ids]
            if available:
                selected = available[0]
                used_image_ids.add(selected['id'])

        assert len(used_image_ids) == 3  # All unique images used


class TestCaching:
    """Test caching mechanisms throughout the pipeline."""

    def test_image_cache_hit(self):
        """Test that cached images are reused within TTL."""
        from datetime import datetime, timedelta

        cache_entry = {
            'images': [{'id': '1', 'url': 'cached.jpg'}],
            'timestamp': datetime.now().isoformat(),
            'query': 'technology'
        }

        # Simulate cache check
        cached_at = datetime.fromisoformat(cache_entry['timestamp'])
        age_days = (datetime.now() - cached_at).days

        assert age_days < 7  # Within 7-day TTL
        images = cache_entry['images']
        assert len(images) > 0  # Cache hit

    def test_image_cache_miss_on_expiry(self):
        """Test that expired cache entries are not used."""
        from datetime import datetime, timedelta

        cache_entry = {
            'images': [{'id': '1', 'url': 'cached.jpg'}],
            'timestamp': (datetime.now() - timedelta(days=8)).isoformat(),
            'query': 'technology'
        }

        # Simulate cache check
        cached_at = datetime.fromisoformat(cache_entry['timestamp'])
        age_days = (datetime.now() - cached_at).days

        if age_days >= 7:
            images = []  # Cache miss, need fresh fetch
        else:
            images = cache_entry['images']

        assert len(images) == 0  # Expired, should refetch


class TestRateLimiting:
    """Test rate limiting and backoff strategies."""

    def test_rate_limit_backoff(self):
        """Test exponential backoff on rate limit errors."""
        retry_attempt = 0
        max_retries = 3
        base_wait = 1

        for attempt in range(max_retries):
            retry_attempt = attempt
            wait_time = base_wait * (2 ** attempt)  # Exponential backoff

            if attempt == 1:  # Simulate success on 2nd retry
                break

        assert retry_attempt == 1  # Succeeded on 2nd attempt
        expected_wait = base_wait * (2 ** 1)
        assert wait_time == expected_wait  # 2 seconds

    def test_provider_exhaustion_tracking(self):
        """Test tracking of exhausted API providers."""
        exhausted_providers = set()

        # Simulate provider failures
        providers = ['groq', 'openrouter', 'google']

        for provider in providers:
            # Simulate rate limit on groq
            if provider == 'groq':
                exhausted_providers.add(provider)
                continue
            else:
                # Success on openrouter
                break

        assert 'groq' in exhausted_providers
        assert len(exhausted_providers) < len(providers)  # Not all exhausted


class TestDeduplication:
    """Test trend deduplication logic."""

    def test_similar_titles_deduped(self):
        """Test that similar titles are deduplicated."""
        from difflib import SequenceMatcher

        trend1 = "Apple announces new iPhone 15 Pro"
        trend2 = "Apple announces new iPhone 15 Pro Max"

        similarity = SequenceMatcher(None, trend1.lower(), trend2.lower()).ratio()

        assert similarity > 0.8  # Should be considered duplicates

    def test_different_titles_kept(self):
        """Test that different titles are not deduplicated."""
        from difflib import SequenceMatcher

        trend1 = "Apple announces new iPhone"
        trend2 = "Google releases Android update"

        similarity = SequenceMatcher(None, trend1.lower(), trend2.lower()).ratio()

        assert similarity < 0.8  # Should be kept separate


class TestFileOperations:
    """Test file creation and management."""

    def test_creates_required_directories(self):
        """Test that pipeline creates necessary directories."""
        with tempfile.TemporaryDirectory() as tmpdir:
            public_dir = Path(tmpdir) / "public"
            data_dir = Path(tmpdir) / "data"
            archive_dir = public_dir / "archive"

            # Simulate directory creation
            public_dir.mkdir(exist_ok=True)
            data_dir.mkdir(exist_ok=True)
            archive_dir.mkdir(exist_ok=True)

            assert public_dir.exists()
            assert data_dir.exists()
            assert archive_dir.exists()

    def test_json_serialization(self):
        """Test that data structures can be serialized to JSON."""
        from datetime import datetime
        from dataclasses import dataclass, asdict

        @dataclass
        class TestTrend:
            title: str
            source: str
            score: float

        trend = TestTrend(title="Test", source="test", score=100.0)
        trend_dict = asdict(trend)

        # Should be JSON serializable
        json_str = json.dumps(trend_dict)
        restored = json.loads(json_str)

        assert restored['title'] == trend.title
        assert restored['score'] == trend.score


class TestArchiveManagement:
    """Test archive creation and cleanup."""

    def test_archive_creation(self):
        """Test that daily archives are created with correct naming."""
        from datetime import datetime

        archive_name = datetime.now().strftime('%Y-%m-%d')
        expected_format = r'^\d{4}-\d{2}-\d{2}$'  # YYYY-MM-DD

        import re
        assert re.match(expected_format, archive_name)

    def test_archive_cleanup_keeps_recent(self):
        """Test that archives within retention period are kept."""
        from datetime import datetime, timedelta

        archive_dates = [
            datetime.now() - timedelta(days=i)
            for i in range(40)  # 40 days of archives
        ]

        # Keep archives < 30 days old
        kept = [d for d in archive_dates if (datetime.now() - d).days < 30]

        assert len(kept) == 30  # Keeps 30 days
        assert len(kept) < len(archive_dates)  # Removes old ones


class TestEditorialGeneration:
    """Test editorial article generation."""

    def test_identifies_central_themes(self):
        """Test that central themes are extracted from trends."""
        trends = [
            {'title': 'AI breakthrough in healthcare', 'keywords': ['ai', 'healthcare']},
            {'title': 'AI revolutionizes education', 'keywords': ['ai', 'education']},
            {'title': 'New AI model released', 'keywords': ['ai', 'model']},
        ]

        # Simulate theme identification
        keyword_freq = {}
        for trend in trends:
            for kw in trend['keywords']:
                keyword_freq[kw] = keyword_freq.get(kw, 0) + 1

        top_theme = max(keyword_freq.items(), key=lambda x: x[1])

        assert top_theme[0] == 'ai'
        assert top_theme[1] == 3  # Appears in all trends

    def test_why_this_matters_for_top_stories(self):
        """Test that 'Why This Matters' is generated for top 3 stories."""
        trends = [
            {'title': 'Story 1', 'score': 150},
            {'title': 'Story 2', 'score': 120},
            {'title': 'Story 3', 'score': 100},
            {'title': 'Story 4', 'score': 80},
        ]

        top_3 = sorted(trends, key=lambda x: x['score'], reverse=True)[:3]

        assert len(top_3) == 3
        assert all(t['score'] >= 100 for t in top_3)
